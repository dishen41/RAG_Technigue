from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from dotenv import load_dotenv

# Define the persist directory
persist_directory = "db/chroma_db_bge"

# Load Model
embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Load the Chroma Vector Store
db = Chroma(
    persist_directory=persist_directory,
    embedding_function=embedding_model,
    collection_metadata={"hnsw:space": "cosine"}
)

# Search for relevant documents
query = "What was Microsoft's first hardware product release?"

# Create retriever
retriever = db.as_retriever(
    search_kwargs={"k":5} #k=5 means only return most similat top 3 Document
)


relevant_docs = retriever.invoke(query)

print(f"User Query: {query}")

# Display results
print("\n--- Context ---")
for i, doc in enumerate(relevant_docs, 1):
    print(f"Document {i}:\n{doc.page_content}\n")

# Questions:

#1. "What was NVIDIA's first graphics accelerator called?"
#2. "Which company did NVIDIA acquire to enter the mobile processor market?"
#3. "What was Microsoft's first hardware product release?"
#4. "How much did Microsoft pay to acquire GitHub?"
#5. "In what year did Tesla begin production of the Roadster?"
#6. "Who succeeded Ze'ev Drori as CEO in October 2008?"
#7. "What was the name of the autonomous spaceport drone ship that achieved the first successful sea landing?"
#8. "What was the original name of Microsoft before it became Microsoft?"
